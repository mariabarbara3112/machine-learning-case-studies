{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Explore the Data\n",
    "\n",
    "We'll be loading in some data about global energy consumption, collected over a few years. The below cell downloads and unzips this data, giving you one text file of data, `household_power_consumption.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget https://s3.amazonaws.com/video.udacity-data.com/topher/2019/March/5c88a3f1_household-electric-power-consumption/household-electric-power-consumption.zip\n",
    "! unzip household-electric-power-consumption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in the `.txt` File\n",
    "\n",
    "The next cell displays the first few lines in the text file, so we can see how it is formatted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display first ten lines of text data\n",
    "n_lines = 10\n",
    "\n",
    "with open('household_power_consumption.txt') as file:\n",
    "    head = [next(file) for line in range(n_lines)]\n",
    "    \n",
    "display(head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Process the Data\n",
    "\n",
    "The 'household_power_consumption.txt' file has the following attributes:\n",
    "   * Each data point has a date and time (hour:minute:second) of recording\n",
    "   * The various data features are separated by semicolons (;)\n",
    "   * Some values are 'nan' or '?', and we'll treat these both as `NaN` values\n",
    "\n",
    "### Managing `NaN` values\n",
    "\n",
    "This DataFrame does include some data points that have missing values. So far, we've mainly been dropping these values, but there are other ways to handle `NaN` values, as well. One technique is to just fill the missing column values with the **mean** value from that column; this way the added value is likely to be realistic.\n",
    "\n",
    "I've provided some helper functions in `txt_preprocessing.py` that will help to load in the original text file as a DataFrame *and* fill in any `NaN` values, per column, with the mean feature value. This technique will be fine for long-term forecasting; if I wanted to do an hourly analysis and prediction, I'd consider dropping the `NaN` values or taking an average over a small, sliding window rather than an entire column of data.\n",
    "\n",
    "**Below, I'm reading the file in as a DataFrame and filling `NaN` values with feature-level averages.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import txt_preprocessing as pprocess\n",
    "\n",
    "# create df from text file\n",
    "initialdf = pprocess.create_df('household_power_consumption.txt', sep=';')\n",
    "\n",
    "# fill NaN column values with *average* column value\n",
    "helperdf = pprocess.fill_nan_with_mean(initialdf)\n",
    "\n",
    "# print some stats about the data\n",
    "print('Data shape: ', helperdf.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Active Power \n",
    "\n",
    "In this example, we'll want to predict the global active power, which is the household minute-averaged active power (kilowatt), measured across the globe. So, below, I am getting just that column of data and displaying the resultant plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select Global active power data\n",
    "powerdf = helperdf['Global_active_power'].copy()\n",
    "print(powerdf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the data \n",
    "plt.figure(figsize=(12,6))\n",
    "# all data points\n",
    "powerdf.plot(title='Global active power', color='black') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the data is recorded each minute, the above plot contains *a lot* of values. So, I'm also showing just a slice of data, below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can plot a slice of hourly data\n",
    "timeslice = 1440 # 1440 mins = 1 day\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "powerdf[0:timeslice].plot(title='Global active power, over one day', color='black') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hourly vs Daily\n",
    "\n",
    "There is a lot of data, collected every minute, and so I could go one of two ways with my analysis:\n",
    "1. Create many, short time series, say a week or so long, in which I record energy consumption every hour, and try to predict the energy consumption over the following hours or days.\n",
    "2. Create fewer, long time series with data recorded daily that I could use to predict usage in the following weeks or months.\n",
    "\n",
    "Both tasks are interesting! It depends on whether you want to predict time patterns over a day/week or over a longer time period, like a month. With the amount of data I have, I think it would be interesting to see longer, *recurring* trends that happen over several months or over a year. So, I will resample the 'Global active power' values, recording **daily** data points as averages over 24-hr periods.\n",
    "\n",
    "> I can resample according to a specified frequency, by utilizing pandas [time series tools](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html), which allow me to sample at points like every hour ('H') or day ('D'), etc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resample over day (D)\n",
    "frequency = 'D'\n",
    "# calculate the mean active power for a day\n",
    "meanpowerdf=powerdf.resample(frequency).mean #frequency is a column in this case 'daily'\n",
    "\n",
    "# display the mean values\n",
    "plt.figure(figsize=(15,8))\n",
    "meanpowerdf.plot(title='Global active power, mean per day', color='black') \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this plot, we can see that there are some interesting trends that occur over each year. It seems that there are spikes of energy consumption around the end/beginning of each year, which correspond with heat and light usage being higher in winter months. We also see a dip in usage around August, when global temperatures are typically higher.\n",
    "\n",
    "The data is still not very smooth, but it shows noticeable trends, and so, makes for a good use case for machine learning models that may be able to recognize these patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Create Time Series \n",
    "\n",
    "My goal will be to take full years of data, from 2007-2009, and see if I can use it to accurately predict the average Global active power usage for the next several months in 2010!\n",
    "\n",
    "Next, let's make one time series for each complete year of data. This is just a design decision, and I am deciding to use full years of data, starting in January of 2017 because there are not that many data points in 2006 and this split will make it easier to handle leap years; I could have also decided to construct time series starting at the first collected data point, just by changing `t_start` and `t_end` in the function below.\n",
    "\n",
    "The function `make_time_series` will create pandas `Series` for each of the passed in list of years `['2007', '2008', '2009']`.\n",
    "* All of the time series will start at the same time point `t_start` (or t0). \n",
    "    * When preparing data, it's important to use a consistent start point for each time series; DeepAR uses this time-point as a frame of reference, which enables it to learn recurrent patterns e.g. that weekdays behave differently from weekends or that Summer is different than Winter.\n",
    "    * You can change the start and end indices to define any time series you create.\n",
    "* We should account for leap years, like 2008, in the creation of time series.\n",
    "* Generally, we create `Series` by getting the relevant global consumption data (from the DataFrame) and date indices.\n",
    "\n",
    "```\n",
    "# get global consumption data\n",
    "data = mean_power_df[start_idx:end_idx]\n",
    "\n",
    "# create time series for the year\n",
    "index = pd.DatetimeIndex(start=t_start, end=t_end, freq='D')\n",
    "time_series.append(pd.Series(data=data, index=index))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_time_series(mmeanpowerdf, years, frequency='D', startindex=16):\n",
    "\n",
    "    timeseries = []\n",
    "    leapyear = '2008'\n",
    "\n",
    "    for y in range(len(years)):\n",
    "\n",
    "        yr = years[y]\n",
    "        if(year == leapyear):\n",
    "            endindex = startindex+366\n",
    "        else:\n",
    "            endindex = startindex+365\n",
    "\n",
    "        starttime = yr  + '-01-01' # Jan 1st \n",
    "        endtime = yr + '-12-31' # Dec 31st \n",
    "\n",
    "        globalconsumption = meanpowerdf[startindex:endindex]\n",
    "\n",
    "        pd.DatetimeIndex(start=startindex, end=endindex, freq=frequency)\n",
    "        timeseries.append(pd.Series(data=timeseries, index=idx))\n",
    "        \n",
    "        startindex=endindex\n",
    "        \n",
    "    return timeseries\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the results\n",
    "\n",
    "Below, let's construct one time series for each complete year of data, and display the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years=['2007', '2008', '2009']\n",
    "frequency='D'\n",
    "timeseries=make_time_series(meanpowerdf, years,freq=frequency) # function defined dabove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display first time series\n",
    "tsi = 0\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "timeseries[tsi].plot(label='test', lw=3, color='black')\n",
    "timeseries[tsi].plot(label='train', ls=':', color='black')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Splitting in Time\n",
    "\n",
    "We'll evaluate our model on a test set of data. For machine learning tasks like classification, we typically create train/test data by randomly splitting examples into different sets. For forecasting it's important to do this train/test split in **time** rather than by individual data points. \n",
    "> In general, we can create training data by taking each of our *complete* time series and leaving off the last `prediction_length` data points to create *training* time series. \n",
    "\n",
    "### EXERCISE: Create training time series\n",
    "\n",
    "Complete the `create_training_series` function, which should take in our list of complete time series data and return a list of truncated, training time series.\n",
    "\n",
    "* In this example, we want to predict about a month's worth of data, and we'll set `prediction_length` to 30 (days).\n",
    "* To create a training set of data, we'll leave out the last 30 points of *each* of the time series we just generated, so we'll use only the first part as training data. \n",
    "* The **test set contains the complete range** of each time series.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create truncated, training time series\n",
    "def create_training_series(timeserieslist, nrpointstopredict):\n",
    "    trainserieslist=[]\n",
    "    for s in timeserieslist:\n",
    "        trainserieslist.append(s[:-nrpointstopredict])\n",
    "        \n",
    "        return: trainserieslist\n",
    "       \n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionlength = 30 # 30 days\n",
    "trainseries = create_training_series(timeserieslist, predictionlength)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Test Series\n",
    "\n",
    "We can visualize what these series look like, by plotting the train/test series on the same axis. We should see that the test series contains all of our data in a year, and a training series contains all but the last `prediction_length` points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display train/test time series\n",
    "tsindex = 0\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "# test data is the whole time series\n",
    "timeseries[tsindex].plot(label='test', lw=3, color=black)\n",
    "# train data is all but the last prediction pts\n",
    "trainseries[tsindex].plot(label='train', ls=':', lw=3, color='black')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to JSON \n",
    "\n",
    "According to the [DeepAR documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/deepar.html), DeepAR expects to see input training data in a JSON format, with the following fields:\n",
    "\n",
    "* **start**: A string that defines the starting date of the time series, with the format 'YYYY-MM-DD HH:MM:SS'.\n",
    "* **target**: An array of numerical values that represent the time series.\n",
    "* **cat** (optional): A numerical array of categorical features that can be used to encode the groups that the record belongs to. This is useful for finding models per class of item, such as in retail sales, where you might have {'shoes', 'jackets', 'pants'} encoded as categories {0, 1, 2}.\n",
    "\n",
    "The input data should be formatted with one time series per line in a JSON file. Each line looks a bit like a dictionary, for example:\n",
    "```\n",
    "{\"start\":'2007-01-01 00:00:00', \"target\": [2.54, 6.3, ...], \"cat\": [1]}\n",
    "{\"start\": \"2012-01-30 00:00:00\", \"target\": [1.0, -5.0, ...], \"cat\": [0]} \n",
    "...\n",
    "```\n",
    "In the above example, each time series has one, associated categorical feature and one time series feature.\n",
    "\n",
    "### EXERCISE: Formatting Energy Consumption Data\n",
    "\n",
    "For our data:\n",
    "* The starting date, \"start,\" will be the index of the first row in a time series, Jan. 1st of that year.\n",
    "* The \"target\" will be all of the energy consumption values that our time series holds.\n",
    "* We will not use the optional \"cat\" field.\n",
    "\n",
    "Complete the following utility function, which should convert `pandas.Series` objects into the appropriate JSON strings that DeepAR can consume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def series_to_json_obj(ts):\n",
    "    tsdictjson={'start':str(ts.index[0],'target':list(ts))}\n",
    "    \n",
    "    return tsdictjson\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test out the code\n",
    "series=timeseries[0]\n",
    "tsjson=series_to_json_obj(ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Data, Locally\n",
    "\n",
    "The next helper function will write one series to a single JSON line, using the new line character '\\n'. The data is also encoded and written to a filename that we specify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json for formatting data\n",
    "import json\n",
    "import os\n",
    "\n",
    "def write_json_dataset(timeseries, filename):\n",
    "    for s in timeseries:\n",
    "        jsonline=json.dumps(s)+'\\n'\n",
    "        jsonline=json:line.encode('utf-8')\n",
    "        f.write(jsonline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save this data to a local directory\n",
    "dir = 'json_energy_data'\n",
    "    \n",
    "if not os.path.exists(dir):\n",
    "    os.makedirs(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directories to save train/test data\n",
    "\n",
    "trainfile=os.path.join(dir, 'train.json')\n",
    "testfile=os.path.join(dir,'test.json')\n",
    "\n",
    "write_json_dataset(trainseries,trainfile)\n",
    "write_json_dataset(timeseries,testfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Uploading Data to S3\n",
    "\n",
    "Next, to make this data accessible to an estimator, I'll upload it to S3.\n",
    "\n",
    "### Sagemaker resources\n",
    "\n",
    "Let's start by specifying:\n",
    "* The sagemaker role and session for training a model.\n",
    "* A default S3 bucket where we can save our training, test, and model data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# session, role, bucket\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE: Upoad *both* training and test JSON files to S3\n",
    "\n",
    "Specify *unique* train and test prefixes that define the location of that data in S3.\n",
    "* Upload training data to a location in S3, and save that location to `train_path`\n",
    "* Upload test data to a location in S3, and save that location to `test_path`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suggested that you set prefixes for directories in S3\n",
    "trainprefix='{}/{}'.format(prefix, 'train')\n",
    "testprefix='{}/{}'.format(prefix,'test''{}/{}'.format(prefix, 'train')\n",
    "# upload data to S3, and save unique locations\n",
    "trainpath = sagemaker_session.upload_data(trainfile, bucket=bucket, key_prefix=trainprefix)\n",
    "testpath = sagemaker_session.upload_data(testfile, bucket=bucket,key_prefix=testprefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check locations\n",
    "print('Training data is stored in: '+ trainpath)\n",
    "print('Test data is stored in: '+ testpath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Training a DeepAR Estimator\n",
    "\n",
    "Some estimators have specific, SageMaker constructors, but not all. Instead you can create a base `Estimator` and pass in the specific image (or container) that holds a specific model.\n",
    "\n",
    "Next, we configure the container image to be used for the region that we are running in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "from sagemaker.amazon.amazon-estimator import et_image_uri\n",
    "img=get_image_uri(boto3.Session().region_name\n",
    "             'forecasting-deepar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE: Instantiate an Estimator \n",
    "\n",
    "You can now define the estimator that will launch the training job. A generic Estimator will be defined by the usual constructor arguments and an `image_name`. \n",
    "> You can take a look at the [estimator source code](https://github.com/aws/sagemaker-python-sdk/blob/master/src/sagemaker/estimator.py#L595) to view specifics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "# instantiate a DeepAR estimator\n",
    "outdirs3=\"s3://{}/{}/output\".format(bucket, prefix)\n",
    "estimator = Estimator(sagemaker_session=sagemaker_session,\n",
    "                      image_name=image_name,\n",
    "                      role=role,\n",
    "                      train_instance_count=1,\n",
    "                      train_instance_type='ml.c4.xlarge',\n",
    "                      output_path=outdirs3\n",
    "                      )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Hyperparameters\n",
    "\n",
    "Next, we need to define some DeepAR hyperparameters that define the model size and training behavior. Values for the epochs, frequency, prediction length, and context length are required.\n",
    "\n",
    "* **epochs**: The maximum number of times to pass over the data when training.\n",
    "* **time_freq**: The granularity of the time series in the dataset ('D' for daily).\n",
    "* **prediction_length**: A string; the number of time steps (based off the unit of frequency) that the model is trained to predict. \n",
    "* **context_length**: The number of time points that the model gets to see *before* making a prediction. \n",
    "\n",
    "### Context Length\n",
    "\n",
    "Typically, it is recommended that you start with a `context_length`=`prediction_length`. This is because a DeepAR model also receives \"lagged\" inputs from the target time series, which allow the model to capture long-term dependencies. For example, a daily time series can have yearly seasonality and DeepAR automatically includes a lag of one year. So, the context length can be shorter than a year, and the model will still be able to capture this seasonality. \n",
    "\n",
    "The lag values that the model picks depend on the frequency of the time series. For example, lag values for daily frequency are the previous week, 2 weeks, 3 weeks, 4 weeks, and year. You can read more about this in the [DeepAR \"how it works\" documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/deepar_how-it-works.html).\n",
    "\n",
    "### Optional Hyperparameters\n",
    "\n",
    "You can also configure optional hyperparameters to further tune your model. These include parameters like the number of layers in our RNN model, the number of cells per layer, the likelihood function, and the training options, such as batch size and learning rate. \n",
    "\n",
    "For an exhaustive list of all the different DeepAR hyperparameters you can refer to the DeepAR [hyperparameter documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/deepar_hyperparameters.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency='D'\n",
    "\n",
    "ctxtlenth=30\n",
    "\n",
    "hyperparameters={\n",
    "    \"epochs\": \"100\",\n",
    "    \"time_freq\": frequency,\n",
    "    \"prediction_length\": str(predictionlength),\n",
    "    \"context_length\": str(ctxtlenth),\n",
    "    \"num_cells\": \"50\",\n",
    "    \"num_layers\": \"2\",\n",
    "    \"mini_batch_size\": \"128\",\n",
    "    \"learning_rate\": \"0.05\",\n",
    "    \"early_stopping_patience\": \"10\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the hyperparams\n",
    "estimator.set_hyperparameters(**hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Job\n",
    "\n",
    "Now, we are ready to launch the training job! SageMaker will start an EC2 instance, download the data from S3, start training the model and save the trained model.\n",
    "\n",
    "If you provide the `test` data channel, as we do in this example, DeepAR will also calculate accuracy metrics for the trained model on this test data set. This is done by predicting the last `prediction_length` points of each time series in the test set and comparing this to the *actual* value of the time series. The computed error metrics will be included as part of the log output.\n",
    "\n",
    "The next cell may take a few minutes to complete, depending on data size, model complexity, and training options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "datapaths={\n",
    "    'train':trainpath\n",
    "    'test': testpath\n",
    "}\n",
    "\n",
    "estimator.fit(inputs=datapaths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy and Create a Predictor\n",
    "\n",
    "Now that we have trained a model, we can use it to perform predictions by deploying it to a predictor endpoint.\n",
    "\n",
    "Remember to **delete the endpoint** at the end of this notebook. A cell at the very bottom of this notebook will be provided, but it is always good to keep, front-of-mind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#deployed model = predictor\n",
    "deployeddeeparmodel=estimator.deploy(\n",
    "    initial_instance_count=1, instance_type='ml.t2.medium', content_type=\"application/json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Generating Predictions\n",
    "\n",
    "According to the [inference format](https://docs.aws.amazon.com/sagemaker/latest/dg/deepar-in-formats.html) for DeepAR, the `predictor` expects to see input data in a JSON format, with the following keys:\n",
    "* **instances**: A list of JSON-formatted time series that should be forecast by the model.\n",
    "* **configuration** (optional): A dictionary of configuration information for the type of response desired by the request.\n",
    "\n",
    "Within configuration the following keys can be configured:\n",
    "* **num_samples**: An integer specifying the number of samples that the model generates when making a probabilistic prediction.\n",
    "* **output_types**: A list specifying the type of response. We'll ask for **quantiles**, which look at the list of num_samples generated by the model, and generate [quantile estimates](https://en.wikipedia.org/wiki/Quantile) for each time point based on these values.\n",
    "* **quantiles**: A list that specified which quantiles estimates are generated and returned in the response.\n",
    "\n",
    "\n",
    "Below is an example of what a JSON query to a DeepAR model endpoint might look like.\n",
    "\n",
    "```\n",
    "{\n",
    " \"instances\": [\n",
    "  { \"start\": \"2009-11-01 00:00:00\", \"target\": [4.0, 10.0, 50.0, 100.0, 113.0] },\n",
    "  { \"start\": \"1999-01-30\", \"target\": [2.0, 1.0] }\n",
    " ],\n",
    " \"configuration\": {\n",
    "  \"num_samples\": 50,\n",
    "  \"output_types\": [\"quantiles\"],\n",
    "  \"quantiles\": [\"0.5\", \"0.9\"]\n",
    " }\n",
    "}\n",
    "```\n",
    "\n",
    "\n",
    "## JSON Prediction Request\n",
    "\n",
    "The code below accepts a **list** of time series as input and some configuration parameters. It then formats that series into a JSON instance and converts the input into an appropriately formatted JSON_input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_predictor_input(inputseries, nrsamples=50, quantiles=['0.1', '0.5', '0.9']):\n",
    "    \n",
    "    helperjsonlist=[]\n",
    "    for pt in range(len(inputseries)):\n",
    "        helperjsonlist.append(inputseries[pt])\n",
    "        \n",
    "    outputconfiguration={\"num_samples\": num_samples, \n",
    "                     \"output_types\": [\"quantiles\"], \n",
    "                     \"quantiles\": quantiles}\n",
    "    outputinfo={\"instances\": instances, \n",
    "                    \"configuration\": configuration}\n",
    "    jsonoutput=json.dumps(outputinfo).encode('utf-8')\n",
    "    \n",
    "    return jsonoutput                     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a Prediction\n",
    "\n",
    "We can then use this function to get a prediction for a formatted time series!\n",
    "\n",
    "In the next cell, I'm getting an input time series and known target, and passing the formatted input into the predictor endpoint to get a resultant prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target: An array of numerical values that represent the time series.\n",
    "targetseries=timeseries\n",
    "\n",
    "modelinputjson=json_predictor_input(trainseries) #function defined above\n",
    "predjson=deployeddeeparmodel.predict(modelinputjson)predjson\n",
    "\n",
    "print(predjson)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding Predictions\n",
    "\n",
    "The predictor returns JSON-formatted prediction, and so we need to extract the predictions and quantile data that we want for visualizing the result. The function below, reads in a JSON-formatted prediction and produces a list of predictions in each quantile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to decode JSON prediction\n",
    "def decode_prediction(prediction, encoding='utf-8'):\n",
    "    #Accepts a JSON prediction and returns a list of prediction data.\n",
    "    prediction_data = json.loads(prediction.decode(encoding))\n",
    "    prediction_list = []\n",
    "    for k in range(len(prediction_data['predictions'])):\n",
    "        prediction_list.append(pd.DataFrame(data=prediction_data['predictions'][k]['quantiles']))\n",
    "    return prediction_list\n",
    "\n",
    "def decode_prediction(predictioninput, encoding='utf-8'):\n",
    "    predicted=json.loads(predictioninput.decode(encoding))\n",
    "    helperpredlst=[]\n",
    "    for idx in range(length(predicted['predictions'])):\n",
    "        helperpredlst.append(pd.DataFrame(data=predicted['predictions'][idx]['quantiles']))\n",
    "        \n",
    "        return helperpredlst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionlist = decode_prediction(predjson)\n",
    "print(predictionlist[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display the Results!\n",
    "\n",
    "The quantile data will give us all we need to see the results of our prediction.\n",
    "* Quantiles 0.1 and 0.9 represent higher and lower bounds for the predicted values.\n",
    "* Quantile 0.5 represents the median of all sample predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the prediction median against the actual data\n",
    "def display_quantiles(len(predictionlist, targetseries)):\n",
    "    for idx in range(len(predictionlist)):\n",
    "        plt.figure(figsize=(12,6))\n",
    "        if targetseries is not None:\n",
    "            target=targetseries[idx][-predictionlength:]\n",
    "            plt.plot(range(len(target)), target, label='target' color='k')\n",
    "            p10=predictionlist[idx]['0.1']\n",
    "            p90=predictionlist[idx]['0.9']\n",
    "            plt.plot(range(len(target)), p10, label='p10' color='k')\n",
    "            plt.plot(range(len(target)), p90, label='p90' color='k')\n",
    "            predictionlist[idx]['0.5'.plot(label='prediction median', color='k']\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display predictions\n",
    "display_quantiles(predictionlist, targetseries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting the Future\n",
    "\n",
    "Recall that we did not give our model any data about 2010, but let's see if it can predict the energy consumption given **no target**, only a known start date!\n",
    "\n",
    "### EXERCISE: Format a request for a \"future\" prediction\n",
    "\n",
    "Create a formatted input to send to the deployed `predictor` passing in my usual parameters for \"configuration\". The \"instances\" will, in this case, just be one instance, defined by the following:\n",
    "* **start**: The start time will be time stamp that you specify. To predict the first 30 days of 2010, start on Jan. 1st, '2010-01-01'.\n",
    "* **target**: The target will be an empty list because this year has no, complete associated time series; we specifically withheld that information from our model, for testing purposes.\n",
    "```\n",
    "{\"start\": start_time, \"target\": []} # empty target\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startdate = '2010-01-01'\n",
    "timept = '00:00:00'\n",
    "starttime = startdate +' '+ timept\n",
    "\n",
    "requestdata={'instances':[{'start':starttime,'target':[]}],\n",
    "             'configuration': {'nrsamples': 50,\n",
    "                               'output_types':['quantiles'],\n",
    "                               'quantiles': ['o.q','0.5', '0.9']}\n",
    "            }\n",
    "\n",
    "predinputjson=json.dumps(requestdata).encode('utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then get and decode the prediction response, as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get prediction response\n",
    "predjson=deployeddeeparmodel.predict(predinputjson)\n",
    "pred2010=decode_prediction(predjson)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, I'll compare the predictions to a known target sequence. This target will come from a time series for the 2010 data, which I'm creating below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tseries2010=[]\n",
    "df2010=meanpowerdf.values[1112:]\n",
    "idx=pd.DatetimeIndex(start=startdate, periods=len(df2010),freq='D')\n",
    "tseries2010.append(pd.Series(data=df2010,index=idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startindex=0\n",
    "endindex=startindex+prediction_length # hyperparameter\n",
    "targetseries2010=[tseries2010[0][startindex:endindex]]\n",
    "display_quantiles(pred2010,targetseries2010)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete the Endpoint\n",
    "\n",
    "Try your code out on different time series. You may want to tweak your DeepAR hyperparameters and see if you can improve the performance of this predictor.\n",
    "\n",
    "When you're done with evaluating the predictor (any predictor), make sure to delete the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: delete the endpoint\n",
    "deployeddeeparmodel.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Now you've seen one complex but far-reaching method for time series forecasting. You should have the skills you need to apply the DeepAR model to data that interests you!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_mxnet_p36",
   "language": "python",
   "name": "conda_amazonei_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "notice": "None."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
