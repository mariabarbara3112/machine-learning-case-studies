{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "First, import the usual resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm storing my **SageMaker variables** in the next cell:\n",
    "* sagemaker_session: The SageMaker session we'll use for training models.\n",
    "* bucket: The name of the default S3 bucket that we'll use for data storage.\n",
    "* role: The IAM role that defines our data and model permissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# sagemaker session, role\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# S3 bucket name\n",
    "bucket = sagemaker_session.default_bucket()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Exploring the Data\n",
    "\n",
    "Next, I am loading the data and unzipping the data in the file `creditcardfraud.zip`. This directory will hold one csv file of all the transaction data, `creditcard.csv`.\n",
    "\n",
    "As in previous notebooks, it's important to look at the distribution of data since this will inform how we develop a fraud detection model. We'll want to know: How many data points we have to work with, the number and type of features, and finally, the distribution of data over the classes (valid or fraudulent)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only have to run once\n",
    "!wget https://s3.amazonaws.com/video.udacity-data.com/topher/2019/January/5c534768_creditcardfraud/creditcardfraud.zip\n",
    "!unzip creditcardfraud\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the csv file\n",
    "localdata = 'creditcard.csv'\n",
    "\n",
    "# print out some data\n",
    "transactiondf = pd.read_csv(localdata)\n",
    "print('Data shape (rows, cols): ', transactiondf.shape)\n",
    "print()\n",
    "transactiondf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE: Calculate the percentage of fraudulent data\n",
    "\n",
    "Take a look at the distribution of this transaction data over the classes, valid and fraudulent. \n",
    "\n",
    "Complete the function `fraudulent_percentage`, below. Count up the number of data points in each class and calculate the *percentage* of the data points that are fraudulent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Calculate the fraction of data points that are fraudulent\n",
    "def fraudulent_percentage(transactiondf):\n",
    "    countdf=transactiondf['Class'].value.counts\n",
    "    regularcounts=countdf[0]\n",
    "    fraudcounts=countdf[1]\n",
    "    percfraud=fraudcounts/(fraudcounts+regularcounts)\n",
    "    return percfraud\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test out your code by calling your function and printing the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# call the function to calculate the fraud percentage\n",
    "fraudperc = fraudulent_percentage(transactiondf)\n",
    "\n",
    "print(fraudperc)\n",
    "print(fraudperc*transactiondf.shape[0])\n",
    "print(transactiondf.shape[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE: Split into train/test datasets\n",
    "\n",
    "In this example, we'll want to evaluate the performance of a fraud classifier; training it on some training data and testing it on *test data* that it did not see during the training process. So, we'll need to split the data into separate training and test sets.\n",
    "\n",
    "Complete the `train_test_split` function, below. This function should:\n",
    "* Shuffle the transaction data, randomly\n",
    "* Split it into two sets according to the parameter `train_frac`\n",
    "* Get train/test features and labels\n",
    "* Return the tuples: (train_features, train_labels), (test_features, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#until now it is not predermined which samples are test and which train\n",
    "np.random.shuffle(transactionmtrixnp)\n",
    "\n",
    "# split into train/test\n",
    "# size of training set determines what of the two parts of the shuffled dataset which transactions are train and which test\n",
    "\n",
    "def train_test_split(transactiondf, trainfrac= 0.7, seed=1):\n",
    "    transactionmatrixnp=transactiondf.asmatrix() #df to np array     \n",
    "    trainfeatures = transactionmatrixnp[:trainsize,-1] #first n columns until trainsize fulfilled, of this las col are labels\n",
    "    trainlabels = transactionmatrixnp[:trainsize, -1] #last column are labels for each row\n",
    "    testfeatures = transactionmatrixnp[trainsize:, -1] #second part after previosly selected part\n",
    "    testlabels = transactionmatrixnp[trainsize:, -1]\n",
    "    \n",
    "    return (trainfeatures, trainlabels), (testfeatures, testlabels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Cell\n",
    "\n",
    "In the cells below, I'm creating the train/test data and checking to see that result makes sense. The tests below test that the above function splits the data into the expected number of points and that the labels are indeed, class labels (0, 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# get train/test data\n",
    "(trainfeatures, trainlabels), (testfeatures, testlabels) = train_test_split(transaction_df, train_frac=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# manual test\n",
    "\n",
    "# for a split of 0.7:0.3 there should be ~2.33x as many training as test pts\n",
    "print('Training data pts: ', len(train_features))\n",
    "print('Test data pts: ', len(test_features))\n",
    "print()\n",
    "\n",
    "# take a look at first item and see that it aligns with first row of data\n",
    "print('First item: \\n', train_features[0])\n",
    "print('Label: ', train_labels[0])\n",
    "print()\n",
    "\n",
    "# test split\n",
    "assert len(train_features) > 2.333*len(test_features), \\\n",
    "        'Unexpected number of train/test points for a train_frac=0.7'\n",
    "# test labels\n",
    "assert np.all(train_labels)== 0 or np.all(train_labels)== 1, \\\n",
    "        'Train labels should be 0s or 1s.'\n",
    "assert np.all(test_labels)== 0 or np.all(test_labels)== 1, \\\n",
    "        'Test labels should be 0s or 1s.'\n",
    "print('Tests passed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Modeling\n",
    "\n",
    "Now that you've uploaded your training data, it's time to define and train a model!\n",
    "\n",
    "In this notebook, you'll define and train the SageMaker, built-in algorithm, [LinearLearner](https://sagemaker.readthedocs.io/en/stable/linear_learner.html). \n",
    "\n",
    "A LinearLearner has two main applications:\n",
    "1. For regression tasks in which a linear line is fit to some data points, and you want to produce a predicted output value given some data point (example: predicting house prices given square area).\n",
    "2. For binary classification, in which a line is separating two classes of data and effectively outputs labels; either 1 for data that falls above the line or 0 for points that fall on or below the line.\n",
    "\n",
    "<img src='notebook_ims/linear_separator.png' width=50% />\n",
    "\n",
    "In this case, we'll be using it for case 2, and we'll train it to separate data into our two classes: valid or fraudulent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE: Create a LinearLearner Estimator\n",
    "\n",
    "You've had some practice instantiating built-in models in SageMaker. All estimators require some constructor arguments to be passed in. See if you can complete this task, instantiating a LinearLearner estimator, using only the [LinearLearner documentation](https://sagemaker.readthedocs.io/en/stable/linear_learner.html) as a resource. This takes in a lot of arguments, but not all are required. My suggestion is to start with a simple model, utilizing default values where applicable. Later, we will discuss some specific hyperparameters and their use cases.\n",
    "\n",
    "#### Instance Types\n",
    "\n",
    "It is suggested that you use instances that are available in the free tier of usage: `'ml.c4.xlarge'` for training and `'ml.t2.medium'` for deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# import LinearLearner\n",
    "from sagemaker import LinearLearner\n",
    "\n",
    "# instantiate LinearLearner\n",
    "linearmodel=Linear(role=role,\n",
    "                       train_instance_count=1, \n",
    "                       train_instance_type='ml.c4.xlarge',\n",
    "                       predictor_type='binary_classifier',\n",
    "                       output_path=output_path,\n",
    "                       sagemaker_session=sagemaker_session,\n",
    "                       epochs=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE: Convert data into a RecordSet format\n",
    "\n",
    "Next, prepare the data for a built-in model by converting the train features and labels into numpy array's of float values. Then you can use the [record_set function](https://sagemaker.readthedocs.io/en/stable/linear_learner.html#sagemaker.LinearLearner.record_set) to format the data as a RecordSet and prepare it for training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "xtrainnp=trainfeatures.astype(float32)\n",
    "ytrainnp=trainlabels(float32)\n",
    "\n",
    "trainmatrixrecordset=linearmodel.record_set(xtrainnp,labels=ytrainnp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE: Train the Estimator\n",
    "\n",
    "After instantiating your estimator, train it with a call to `.fit()`, passing in the formatted training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "linearmodel.fit(trainmatrixrecordset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE: Deploy the trained model\n",
    "\n",
    "Deploy your model to create a predictor. We'll use this to make predictions on our test data and evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "deployedlinearmodel=linearmodel.deploy(initial_instance_count=1, instance_type='ml.t2.medium')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Evaluating Your Model\n",
    "\n",
    "Once your model is deployed, you can see how it performs when applied to the test data.\n",
    "\n",
    "According to the deployed [predictor documentation](https://sagemaker.readthedocs.io/en/stable/linear_learner.html#sagemaker.LinearLearnerPredictor), this predictor expects an `ndarray` of input features and returns a list of Records.\n",
    "> \"The prediction is stored in the \"predicted_label\" key of the `Record.label` field.\"\n",
    "\n",
    "Let's first test our model on just one test point, to see the resulting list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# test one prediction\n",
    "xtestnp = testfeatures.astype('float32')\n",
    "result = deployedlinearmodel.predict(testxnp[0])\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function for evaluation\n",
    "\n",
    "\n",
    "The provided function below, takes in a deployed predictor, some test features and labels, and returns a dictionary of metrics; calculating false negatives and positives as well as recall, precision, and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(predictor, test_features, test_labels, verbose=True): \n",
    "    # split the test data set into batches and evaluate using prediction endpoint    \n",
    "    predictionbatches=[predictor.predict(b) for b in np.array_split(testfeatures, 100)]\n",
    "    \n",
    "    # LinearLearner produces a `predicted_label` for each data point in a batch\n",
    "    # get the 'predicted_label' for every point in a batch\n",
    "    test_preds = np.concatenate([np.array([x.label['predicted_label'].float32_tensor.values[0] for x in batch]) \n",
    "                                 for batch in prediction_batches])\n",
    "    \n",
    "    predictions=np.concatex.label['predicted_label'].float_32-tensor.values[0]nate(np.array[ for p in b] for b in predictionbatches)\n",
    "    \n",
    "    # calculate true positives, false positives, true negatives, false negatives\n",
    "    tp = np.logical_and(test_labels, predictions).sum()\n",
    "    fp = np.logical_and(1-test_labels, predictions).sum()\n",
    "    tn = np.logical_and(1-predictions, 1-test_preds).sum()\n",
    "    fn = np.logical_and(test_labels, 1-predictions).sum()\n",
    "    \n",
    "    # calculate binary classification metrics\n",
    "    recall = tp / (tp + fn)\n",
    "    precision = tp / (tp + fp)\n",
    "    accuracy = (tp + tn) / (tp + fp + tn + fn)\n",
    "    \n",
    "    # printing a table of metrics\n",
    "    if verbose:\n",
    "        print(pd.crosstab(test_labels, test_preds, rownames=['actual (row)'], colnames=['prediction (col)']))\n",
    "        print(\"\\n{:<11} {:.3f}\".format('Recall:', recall))\n",
    "        print(\"{:<11} {:.3f}\".format('Precision:', precision))\n",
    "        print(\"{:<11} {:.3f}\".format('Accuracy:', accuracy))\n",
    "        print()\n",
    "        \n",
    "    return {'TP': tp, 'FP': fp, 'FN': fn, 'TN': tn, \n",
    "            'Precision': precision, 'Recall': recall, 'Accuracy': accuracy}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Results\n",
    "\n",
    "The cell below runs the `evaluate` function. \n",
    "\n",
    "The code assumes that you have a defined `predictor` and `test_features` and `test_labels` from previously-run cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "print('Metrics for simple, LinearLearner.\\n')\n",
    "\n",
    "# get metrics for linear predictor\n",
    "metrics = evaluate(deployedlinearmodel, \n",
    "                   test_features.astype('float32'), \n",
    "                   test_labels, \n",
    "                   verbose=True) # verbose means we'll print out the metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete the Endpoint\n",
    "\n",
    "I've added a convenience function to delete prediction endpoints after we're done with them. And if you're done evaluating the model, you should delete your model endpoint!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Deletes a precictor.endpoint\n",
    "def delete_endpoint(deployedlinearmodel):\n",
    "        try:\n",
    "            boto3.client('sagemaker').delete_endpoint(EndpointName=predictor.endpoint)\n",
    "            print('Deleted {}'.format(predictor.endpoint))\n",
    "        except:\n",
    "            print('Already deleted: {}'.format(predictor.endpoint))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# delete the predictor endpoint \n",
    "delete_endpoint(linear_predictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improvement: Model Tuning\n",
    "\n",
    "Optimizing according to a specific metric is called **model tuning**, and SageMaker provides a number of ways to automatically tune a model.\n",
    "\n",
    "\n",
    "### Create a LinearLearner and tune for higher precision \n",
    "\n",
    "**Scenario:**\n",
    "* A bank has asked you to build a model that detects cases of fraud with an accuracy of about 85%. \n",
    "\n",
    "In this case, we want to build a model that has as many true positives and as few false negatives, as possible. This corresponds to a model with a high **recall**: true positives / (true positives + false negatives). \n",
    "\n",
    "To aim for a specific metric, LinearLearner offers the hyperparameter `binary_classifier_model_selection_criteria`, which is the model evaluation criteria for the training dataset. A reference to this parameter is in [LinearLearner's documentation](https://sagemaker.readthedocs.io/en/stable/linear_learner.html#sagemaker.LinearLearner). We'll also have to further specify the exact value we want to aim for; read more about the details of the parameters, [here](https://docs.aws.amazon.com/sagemaker/latest/dg/ll_hyperparameters.html).\n",
    "\n",
    "I will assume that performance on a training set will be within about 5% of the performance on a test set. So, for a recall of about 85%, I'll aim for a bit higher, 90%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# instantiate a LinearLearner\n",
    "# tune the model for a higher recall\n",
    "linearrecall = LinearLearner(role=role,\n",
    "                              train_instance_count=1, \n",
    "                              train_instance_type='ml.c4.xlarge',\n",
    "                              predictor_type='binary_classifier',\n",
    "                              output_path=output_path,\n",
    "                              sagemaker_session=sagemaker_session,\n",
    "                              epochs=15,\n",
    "                              binary_classifier_model_selection_criteria='precision_at_target_recall', # target recall\n",
    "                              target_recall=0.9) # 90% recall\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the tuned estimator\n",
    "\n",
    "Fit the new, tuned estimator on the formatted training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "# train the estimator on formatted training data\n",
    "linearrecall.fit(trainmatrixrecordset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy and evaluate the tuned estimator\n",
    "\n",
    "Deploy the tuned predictor and evaluate it.\n",
    "\n",
    "We hypothesized that a tuned model, optimized for a higher recall, would have fewer false negatives (fraudulent transactions incorrectly labeled as valid); did the number of false negatives get reduced after tuning the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "# deploy and create a predictor\n",
    "deployedlinearmodel = linearrecall.deploy(initial_instance_count=1, instance_type='ml.t2.medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "print('Metrics for tuned (recall), LinearLearner.\\n')\n",
    "\n",
    "# get metrics for tuned predictor\n",
    "metrics = evaluate(deployedlinearmodel, \n",
    "                   testfeatures.astype('float32'), \n",
    "                   testlabels, \n",
    "                   verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete the endpoint \n",
    "\n",
    "As always, when you're done evaluating a model, you should delete the endpoint. Below, I'm using the `delete_endpoint` helper function I defined earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# delete the predictor endpoint \n",
    "delete_endpoint(recall_predictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Improvement: Managing Class Imbalance\n",
    "\n",
    "We have a model that is tuned to get a higher recall, which aims to reduce the number of false negatives. Earlier, we discussed how class imbalance may actually bias our model towards predicting that all transactions are valid, resulting in higher false negatives and true negatives. It stands to reason that this model could be further improved if we account for this imbalance.\n",
    "\n",
    "To account for class imbalance during training of a binary classifier, LinearLearner offers the hyperparameter, `positive_example_weight_mult`, which is the weight assigned to positive (1, fraudulent) examples when training a binary classifier. The weight of negative examples (0, valid) is fixed at 1. \n",
    "\n",
    "### EXERCISE: Create a LinearLearner with a `positive_example_weight_mult` parameter\n",
    "\n",
    "In **addition** to tuning a model for higher recall (you may use `linear_recall` as a starting point), you should *add* a parameter that helps account for class imbalance. From the [hyperparameter documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/ll_hyperparameters.html) on `positive_example_weight_mult`, it reads:\n",
    "> \"If you want the algorithm to choose a weight so that errors in classifying negative vs. positive examples have equal impact on training loss, specify `balanced`.\"\n",
    "\n",
    "You could also put in a specific float value, in which case you'd want to weight positive examples more heavily than negative examples, since there are fewer of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# you have to train a new linear model to be able to balance the impact of + and - samples\n",
    "#basically you have to start everything anew\n",
    "balancedmodel=Linear(role=role,\n",
    "                                train_instance_count=1, \n",
    "                                train_instance_type='ml.c4.xlarge',\n",
    "                                predictor_type='binary_classifier',\n",
    "                                output_path=output_path,\n",
    "                                sagemaker_session=sagemaker_session,\n",
    "                                epochs=15,\n",
    "                                binary_classifier_model_selection_criteria='precision_at_target_recall', # target recall\n",
    "                                target_recall=0.9,\n",
    "                                positive_example_weight_mult='balanced')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE: Train the balanced estimator\n",
    "\n",
    "Fit the new, balanced estimator on the formatted training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "# train the estimator on formatted training data\n",
    "#fit=training is always done on training data\n",
    "balancedmodel.fit(balancedmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXERCISE: Deploy and evaluate the balanced estimator\n",
    "\n",
    "Deploy the balanced predictor and evaluate it. Do the results match with your expectations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "# deploy and create a predictor\n",
    "deployedlinearmodel = balancedmodel.deploy(balancedmodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "print('Metrics for balanced, LinearLearner.\\n')\n",
    "\n",
    "# get metrics for balanced predictor\n",
    "metrics = evaluate(deployedlinearmodel, \n",
    "                   test_features.astype('float32'), \n",
    "                   test_labels, \n",
    "                   verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete the endpoint \n",
    "\n",
    "When you're done evaluating a model, you should delete the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# delete the predictor endpoint \n",
    "delete_endpoint(deployedlinearmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A note on metric variability: \n",
    "\n",
    "The above model is tuned for the best possible precision with recall fixed at about 90%. The recall is fixed at 90% during training, but may vary when we apply our trained model to a test set of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Model Design\n",
    "\n",
    "Now that you've seen how to tune and balance a LinearLearner. Create, train and deploy your own model. This exercise is meant to be more open-ended, so that you get practice with the steps involved in designing a model and deploying it.\n",
    "\n",
    "### EXERCISE: Train and deploy a LinearLearner with appropriate hyperparameters, according to the given scenario\n",
    "\n",
    "**Scenario:**\n",
    "* A bank has asked you to build a model that optimizes for a good user experience; users should only ever have up to about 15% of their valid transactions flagged as fraudulent.\n",
    "\n",
    "This requires that you make a design decision: Given the above scenario, what metric (and value) should you aim for during training?\n",
    "\n",
    "You may assume that performance on a training set will be within about 5-10% of the performance on a test set. For example, if you get 80% on a training set, you can assume that you'll get between about 70-90% accuracy on a test set.\n",
    "\n",
    "Your final model should account for class imbalance and be appropriately tuned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#\n",
    "finallinearmodel=Linear(role=role,\n",
    "                                train_instance_count=1, \n",
    "                                train_instance_type='ml.c4.xlarge',\n",
    "                                predictor_type='binary_classifier',\n",
    "                                output_path=output_path,\n",
    "                                sagemaker_session=sagemaker_session,\n",
    "                                epochs=15,\n",
    "                                binary_classifier_model_selection_criteria='recall_at_target_precision',\n",
    "                                target_precision=0.9,\n",
    "                                positive_example_weight_mult='balanced')\n",
    "\n",
    "finallinearmodel.fit(trainmatrixrecordset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "# deploy and evaluate a predictor\n",
    "deployedfinallinearmodel=finallinearmodel.deploy(initial_instance_count=1, instance_type='ml.t2.medium')\n",
    "\n",
    "metrics=evaluate(deployedfinallinearmodel,\n",
    "                 testfeatures.astype('float32'),\n",
    "                 testlabels,\n",
    "                 verbose=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## IMPORTANT\n",
    "# delete the predictor endpoint after evaluation \n",
    "delete_endpoint(deployedfinallinearmodel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Cleanup!\n",
    "\n",
    "* Double check that you have deleted all your endpoints.\n",
    "* I'd also suggest manually deleting your S3 bucket, models, and endpoint configurations directly from your AWS console.\n",
    "\n",
    "You can find thorough cleanup instructions, [in the documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-cleanup.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Conclusion\n",
    "\n",
    "In this notebook, you saw how to train and deploy a LinearLearner in SageMaker. This model is well-suited for a binary classification task that involves specific design decisions and managing class imbalance in the training set.\n",
    "\n",
    "Following the steps of a machine learning workflow, you loaded in some credit card transaction data, explored that data and prepared it for model training. Then trained, deployed, and evaluated several models, according to different design considerations!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_mxnet_p36",
   "language": "python",
   "name": "conda_amazonei_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "notice": "None."
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
